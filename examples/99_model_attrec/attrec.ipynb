{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Recommenders contributors.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attrec\n",
    "\n",
    "Next Item Recommendation with Self-Attention (Shuai Zhang et al.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\n",
      "Tensorflow version: 2.18.0\n",
      "AttRec module imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf.get_logger().setLevel('ERROR')  # only show error messages\n",
    "\n",
    "\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.datasets.python_splitters import python_chrono_split\n",
    "from recommenders.evaluation.python_evaluation import (\n",
    "    map, ndcg_at_k, precision_at_k, recall_at_k\n",
    ")\n",
    "\n",
    "from recommenders.models.attrec.attrec import AttRec\n",
    "from recommenders.models.attrec.dataIterator import DataIterator\n",
    "from recommenders.utils.constants import SEED as DEFAULT_SEED\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(\"AttRec module imported successfully!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 50\n",
    "\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "SEED = DEFAULT_SEED  # Set None for non-deterministic results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:02<00:00, 1.93kKB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3.0</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating  timestamp\n",
       "0     196     242     3.0  881250949\n",
       "1     186     302     3.0  891717742\n",
       "2      22     377     1.0  878887116\n",
       "3     244      51     2.0  880606923\n",
       "4     166     346     1.0  886397596"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_train_test(df, seq_counts=5, target_counts=3, save_dir='processed_data', is_Save=True):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train/test sets with user-item sequences.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the user-item interaction data file.\n",
    "        seq_counts (int): Length of input sequences.\n",
    "        target_counts (int): Number of items to predict.\n",
    "        save_dir (str): Directory to save the train/test data.\n",
    "        is_save (bool): Whether to save the datasets and metadata.\n",
    "\n",
    "    Returns:\n",
    "        train (pd.DataFrame): Training data.\n",
    "        test (pd.DataFrame): Testing data.\n",
    "        user_all_items (dict): Mapping of users to their full item interaction lists.\n",
    "        all_user_count (int): Total number of unique users.\n",
    "        all_item_count (int): Total number of unique items.\n",
    "        user_map (dict): Mapping of original user IDs to remapped IDs.\n",
    "        item_map (dict): Mapping of original item IDs to remapped IDs.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # # Load data\n",
    "    # data_path = '/Users/leeisbadk/recommenders/examples/99_model_attrec/ml-100k/ml-100k/u.data'\n",
    "    # data = pd.read_csv(data_path, sep='\\t', header=None, names=['userID', 'itemID', 'rating', 'timestamp'])\n",
    "    data = df.copy()\n",
    "    # Remap user and item IDs to start from 1\n",
    "    user_map = {uid: i for i, uid in enumerate(data['userID'].unique())}\n",
    "    item_map = {iid: i for i, iid in enumerate(data['itemID'].unique())}\n",
    "    data['userID'] = data['userID'].map(user_map)\n",
    "    data['itemID'] = data['itemID'].map(item_map)\n",
    "\n",
    "    # Sort data by user and timestamp\n",
    "    data = data.sort_values(by=['userID', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "    # Group data by user\n",
    "    user_sessions = data.groupby('userID')['itemID'].apply(list).reset_index()\n",
    "    user_sessions.rename(columns={'itemID': 'item_list'}, inplace=True)\n",
    "\n",
    "    user_all_items = {}\n",
    "    train_users, train_seqs, train_targets = [], [], []\n",
    "    test_users, test_seqs, test_targets = [], [], []\n",
    "\n",
    "    for _, row in user_sessions.iterrows():\n",
    "        user = row['userID']\n",
    "        items = row['item_list']\n",
    "        user_all_items[user] = items\n",
    "\n",
    "        # Create training sequences\n",
    "        for i in range(seq_counts, len(items) - target_counts):\n",
    "            seqs = items[i - seq_counts:i]\n",
    "            targets = items[i:i + target_counts]\n",
    "            train_users.append(user)\n",
    "            train_seqs.append(seqs)\n",
    "            train_targets.append(targets)\n",
    "\n",
    "        # Create testing sequence\n",
    "        if len(items) > seq_counts + target_counts:\n",
    "            test_seq = items[-seq_counts - target_counts:-target_counts]\n",
    "            test_target = items[-target_counts:]\n",
    "            test_users.append(user)\n",
    "            test_seqs.append(test_seq)\n",
    "            test_targets.append(test_target)\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    train = pd.DataFrame({'user': train_users, 'seq': train_seqs, 'target': train_targets})\n",
    "    test = pd.DataFrame({'user': test_users, 'seq': test_seqs, 'target': test_targets})\n",
    "\n",
    "    # Metadata\n",
    "    all_user_count = len(user_map)\n",
    "    all_item_count = len(item_map)\n",
    "\n",
    "    if is_Save:\n",
    "        # Save datasets\n",
    "        train.to_csv(os.path.join(save_dir, 'train.csv'), index=False)\n",
    "        test.to_csv(os.path.join(save_dir, 'test.csv'), index=False)\n",
    "\n",
    "        # Save mappings and metadata\n",
    "        with open(os.path.join(save_dir, 'info.pkl'), 'wb') as f:\n",
    "            pickle.dump(user_all_items, f, pickle.HIGHEST_PROTOCOL)\n",
    "            pickle.dump(all_user_count, f, pickle.HIGHEST_PROTOCOL)\n",
    "            pickle.dump(all_item_count, f, pickle.HIGHEST_PROTOCOL)\n",
    "            pickle.dump(user_map, f, pickle.HIGHEST_PROTOCOL)\n",
    "            pickle.dump(item_map, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print(f\"Train and test datasets saved in '{save_dir}'\")\n",
    "\n",
    "    return train, test, user_all_items, all_user_count, all_item_count, user_map, item_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(file_path='/Users/leeisbadk/Library/Jupyter/runtime/kernel-v3fce7ee20d8b163196fd910f3060ba08818d5a183.json', test_path='input/test.csv', train_path='input/train.csv', mode='train', w=0.3, num_epochs=30, sequence_length=5, target_length=3, neg_sample_count=10, item_count=1685, user_count=945, embedding_size=100, batch_size=256, learning_rate=0.01, keep_prob=0.5, l2_lambda=0.001, gamma=0.5, grad_clip=10, save_path='save_path/model1.ckpt')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--file_path', type=str, default='/Users/leeisbadk/recommenders/examples/99_model_attrec/ml-100k/ml-100k/u.data', help='training data dir')\n",
    "parser.add_argument('--test_path', type=str, default='input/test.csv', help='testing data dir')\n",
    "parser.add_argument('--train_path', type=str, default='input/train.csv', help='training data dir')\n",
    "parser.add_argument('--mode', type=str, default='train', help='train or test')\n",
    "parser.add_argument('--w', type=float, default=0.3, help='The final score is a weighted sum of them with the controlling factor ω')\n",
    "parser.add_argument('--num_epochs', type=int, default=30, help='number of epochs')\n",
    "parser.add_argument('--sequence_length', type=int, default=5, help='sequence length')\n",
    "parser.add_argument('--target_length', type=int, default=3, help='target length') ##ควรเป็น 3\n",
    "parser.add_argument('--neg_sample_count', type=int, default=10, help='number of negative sample')\n",
    "parser.add_argument('--item_count', type=int, default=1685, help='number of items')\n",
    "parser.add_argument('--user_count', type=int, default=945, help='number of user')\n",
    "parser.add_argument('--embedding_size', type=int, default=100, help='embedding size')\n",
    "parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-2, help='learning rate')\n",
    "parser.add_argument('--keep_prob', type=float, default=0.5, help='keep prob of dropout')\n",
    "parser.add_argument('--l2_lambda', type=float, default=1e-3, help='Regularization rate for l2')\n",
    "parser.add_argument('--gamma', type=float, default=0.5, help='gamma of the margin higle loss')\n",
    "parser.add_argument('--grad_clip', type=float, default=10, help='gradient clip to prevent from grdient to large')\n",
    "parser.add_argument('--save_path', type=str, default='save_path/model1.ckpt', help='the whole path to save the model')\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "print(FLAGS)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metric_HR(target_list, predict_list, num):\n",
    "    count = 0\n",
    "    for i in range(len(target_list)):\n",
    "        t = target_list[i]\n",
    "        preds = predict_list[i]\n",
    "        preds = preds[:num]\n",
    "        if t in preds:\n",
    "            count += 1\n",
    "    return count / len(target_list)\n",
    "\n",
    "def Metric_MRR(target_list, predict_list):\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(target_list)):\n",
    "        t = target_list[i]\n",
    "        preds = predict_list[i]\n",
    "        rank = preds.index(t) + 1\n",
    "        count += 1 / rank\n",
    "    return count / len(target_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " make datasets\n",
      "       user                        seq            target\n",
      "0         0    [0, 289, 491, 380, 751]    [466, 522, 10]\n",
      "1         0  [289, 491, 380, 751, 466]    [522, 10, 672]\n",
      "2         0  [491, 380, 751, 466, 522]   [10, 672, 1045]\n",
      "3         0   [380, 751, 466, 522, 10]  [672, 1045, 649]\n",
      "4         0   [751, 466, 522, 10, 672]  [1045, 649, 377]\n",
      "...     ...                        ...               ...\n",
      "92451   942   [209, 10, 873, 935, 614]    [355, 158, 12]\n",
      "92452   942   [10, 873, 935, 614, 355]    [158, 12, 141]\n",
      "92453   942  [873, 935, 614, 355, 158]    [12, 141, 452]\n",
      "92454   942   [935, 614, 355, 158, 12]   [141, 452, 672]\n",
      "92455   942   [614, 355, 158, 12, 141]    [452, 672, 68]\n",
      "\n",
      "[92456 rows x 3 columns]\n",
      "     user                           seq            target\n",
      "0       0    [834, 438, 632, 656, 1006]   [947, 363, 521]\n",
      "1       1       [452, 899, 25, 246, 48]    [530, 145, 31]\n",
      "2       2     [758, 437, 458, 476, 368]    [769, 14, 305]\n",
      "3       3   [834, 215, 1092, 945, 1100]  [689, 1210, 525]\n",
      "4       4     [652, 175, 731, 265, 668]   [187, 252, 985]\n",
      "..    ...                           ...               ...\n",
      "938   938  [190, 634, 1172, 1119, 1067]  [1104, 415, 706]\n",
      "939   939     [534, 1074, 672, 59, 642]   [189, 722, 177]\n",
      "940   940       [276, 221, 6, 199, 389]   [288, 347, 148]\n",
      "941   941     [420, 404, 608, 157, 184]   [117, 585, 139]\n",
      "942   942      [355, 158, 12, 141, 452]     [672, 68, 24]\n",
      "\n",
      "[943 rows x 3 columns]\n",
      " load model and training\n",
      "Tensor(\"AttRec/attention/MatMul_1:0\", shape=(?, 5, 100), dtype=float32)\n",
      "pass\n",
      "Tensor(\"AttRec/Max:0\", shape=(?, 100), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732049187.805846 1737827 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "W0000 00:00:1732049187.921818 1737827 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1732049195.196593 1737827 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 1,  mean_loss0.137309, test HR@50: 0.401909, test MRR: 0.0501173\n",
      " epoch 2,  mean_loss0.0641269, test HR@50: 0.464475, test MRR: 0.0527256\n",
      " epoch 3,  mean_loss0.0547983, test HR@50: 0.469777, test MRR: 0.0611763\n",
      " epoch 4,  mean_loss0.0520617, test HR@50: 0.506893, test MRR: 0.0614353\n",
      " epoch 5,  mean_loss0.0511071, test HR@50: 0.510074, test MRR: 0.0566272\n",
      " epoch 6,  mean_loss0.0500645, test HR@50: 0.520679, test MRR: 0.0620064\n",
      " epoch 7,  mean_loss0.0493421, test HR@50: 0.49947, test MRR: 0.0671659\n",
      " epoch 8,  mean_loss0.0486861, test HR@50: 0.534464, test MRR: 0.06204\n",
      " epoch 9,  mean_loss0.0484701, test HR@50: 0.538706, test MRR: 0.06689\n",
      " epoch 10,  mean_loss0.0480206, test HR@50: 0.52492, test MRR: 0.0681078\n",
      " epoch 11,  mean_loss0.0477576, test HR@50: 0.525981, test MRR: 0.0671324\n",
      " epoch 12,  mean_loss0.047681, test HR@50: 0.510074, test MRR: 0.062927\n",
      " epoch 13,  mean_loss0.0472668, test HR@50: 0.54719, test MRR: 0.0640761\n",
      " epoch 14,  mean_loss0.0472526, test HR@50: 0.544008, test MRR: 0.0630327\n",
      " epoch 15,  mean_loss0.0471445, test HR@50: 0.537646, test MRR: 0.0628821\n",
      " epoch 16,  mean_loss0.0468431, test HR@50: 0.520679, test MRR: 0.0706819\n",
      " epoch 17,  mean_loss0.0469171, test HR@50: 0.527041, test MRR: 0.0686978\n",
      " epoch 18,  mean_loss0.0465455, test HR@50: 0.527041, test MRR: 0.0672758\n",
      " epoch 19,  mean_loss0.0464862, test HR@50: 0.527041, test MRR: 0.0657612\n",
      " epoch 20,  mean_loss0.0464704, test HR@50: 0.52386, test MRR: 0.0656289\n",
      " epoch 21,  mean_loss0.0461114, test HR@50: 0.527041, test MRR: 0.0657565\n",
      " epoch 22,  mean_loss0.0462291, test HR@50: 0.513256, test MRR: 0.0635658\n",
      " epoch 23,  mean_loss0.0460731, test HR@50: 0.512195, test MRR: 0.0638979\n",
      " epoch 24,  mean_loss0.0457141, test HR@50: 0.507953, test MRR: 0.0635423\n",
      " epoch 25,  mean_loss0.0458626, test HR@50: 0.512195, test MRR: 0.0647586\n",
      " epoch 26,  mean_loss0.0455663, test HR@50: 0.514316, test MRR: 0.0641865\n",
      " epoch 27,  mean_loss0.0456186, test HR@50: 0.49947, test MRR: 0.0660338\n",
      " epoch 28,  mean_loss0.0456781, test HR@50: 0.502651, test MRR: 0.0661308\n",
      " epoch 29,  mean_loss0.0452606, test HR@50: 0.494168, test MRR: 0.0626417\n",
      " epoch 30,  mean_loss0.0453589, test HR@50: 0.492047, test MRR: 0.0685312\n"
     ]
    }
   ],
   "source": [
    "from recommenders.models.attrec.attrec import AttRec\n",
    "from recommenders.models.attrec.dataIterator import DataIterator\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "\n",
    "def Metric_HR(target_list, predict_list, num):\n",
    "    count = 0\n",
    "    for i in range(len(target_list)):\n",
    "        t = target_list[i]\n",
    "        preds = predict_list[i]\n",
    "        preds = preds[:num]\n",
    "        if t in preds:\n",
    "            count += 1\n",
    "    return count / len(target_list)\n",
    "\n",
    "def Metric_MRR(target_list, predict_list):\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(target_list)):\n",
    "        t = target_list[i]\n",
    "        preds = predict_list[i]\n",
    "        rank = preds.index(t) + 1\n",
    "        count += 1 / rank\n",
    "    return count / len(target_list)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    data, num_users, num_items = df, df['userID'].nunique(), df['itemID'].nunique()\n",
    "    print(' make datasets')\n",
    "    train_data, test_data ,user_all_items, all_user_count\\\n",
    "        , all_item_count, user_map, item_map \\\n",
    "        = create_train_test(data, FLAGS.sequence_length, FLAGS.target_length, is_Save=False)\n",
    "    FLAGS.item_count = all_item_count\n",
    "    FLAGS.user_count = all_user_count\n",
    "    all_index = [i for i in range(FLAGS.item_count)]\n",
    "    print(train_data)\n",
    "    print(test_data)\n",
    "    print(' load model and training')\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "      with tf.compat.v1.Session() as sess:\n",
    "          #Load model\n",
    "          model = AttRec(FLAGS)\n",
    "          topk_index = model.predict(all_index,len(all_index))\n",
    "          total_loss = model.loss\n",
    "\n",
    "          #Add L2\n",
    "          # with tf.name_scope('l2loss'):\n",
    "          #     loss = model.loss\n",
    "          #     tv = tf.trainable_variables()\n",
    "          #     regularization_cost = FLAGS.l2_lambda * tf.reduce_sum([tf.nn.l2_loss(v) for v in tv])\n",
    "          #     total_loss = loss + regularization_cost\n",
    "\n",
    "          #Optimizer\n",
    "          global_step = tf.Variable(0, trainable=False)\n",
    "          update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "          with tf.control_dependencies(update_ops):\n",
    "              optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "              tvars = tf.trainable_variables()\n",
    "              grads, _ = tf.clip_by_global_norm(tf.gradients(total_loss, tvars), FLAGS.grad_clip)\n",
    "              grads_and_vars = tuple(zip(grads, tvars))\n",
    "              train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "          #Saver and initializer\n",
    "          saver = tf.train.Saver()\n",
    "          if FLAGS.mode == 'test':\n",
    "              saver.restore(sess, FLAGS.save_path)\n",
    "          else:\n",
    "              sess.run(tf.global_variables_initializer())\n",
    "\n",
    "          #Batch reader\n",
    "          trainIterator = DataIterator(data=train_data\n",
    "                                      , batch_size=FLAGS.batch_size\n",
    "                                      ,max_seq_length=FLAGS.batch_size\n",
    "                                      ,neg_count=FLAGS.neg_sample_count\n",
    "                                      ,all_items=all_index\n",
    "                                      ,user_all_items=user_all_items\n",
    "                                      ,shuffle=True)\n",
    "          testIterator = DataIterator(data=test_data\n",
    "                                      ,batch_size = FLAGS.batch_size\n",
    "                                      , max_seq_length=FLAGS.batch_size\n",
    "                                      , neg_count=FLAGS.neg_sample_count\n",
    "                                      , all_items=all_index\n",
    "                                      , user_all_items=user_all_items\n",
    "                                      , shuffle=False)\n",
    "          #Training and test for every epoch\n",
    "          for epoch in range(FLAGS.num_epochs):\n",
    "              cost_list = []\n",
    "              for train_input in trainIterator:\n",
    "                user, next_target, user_seq, sl, neg_seq = train_input\n",
    "\n",
    "                # Convert lists to NumPy arrays before checking shape\n",
    "                user_seq_array = np.array(user_seq)\n",
    "                neg_seq_array = np.array(neg_seq)\n",
    "                user_array = np.array(user)\n",
    "                next_target_array = np.array(next_target)\n",
    "                # Print shapes of relevant tensors\n",
    "                # print(\"Shape of user_seq:\", user_seq_array.shape)\n",
    "                # print(\"Shape of neg_seq:\", neg_seq_array.shape)\n",
    "                # print(\"Shape of hist_seq:\", user_seq_array.shape)  #tis the issue\n",
    "                feed_dict = {model.u_p: user, model.next_p: next_target, model.sl: sl,\n",
    "                            model.hist_seq: user_seq, model.neg_p: neg_seq,\n",
    "                            model.keep_prob:FLAGS.keep_prob,model.is_Training:True}\n",
    "\n",
    "                _, step, cost = sess.run([train_op, global_step, total_loss], feed_dict)\n",
    "                cost_list.append(np.mean(cost))\n",
    "              mean_cost = np.mean(cost_list)\n",
    "              saver.save(sess, FLAGS.save_path)\n",
    "\n",
    "              pred_list = []\n",
    "              next_list = []\n",
    "              # test and cal hr50 and mrr\n",
    "              for test_input in testIterator:\n",
    "                  user, next_target, user_seq, sl, neg_seq = test_input\n",
    "                  feed_dict = {model.u_p: user, model.next_p: next_target, model.sl: sl,\n",
    "                              model.hist_seq: user_seq,model.keep_prob:1.0\n",
    "                              ,model.is_Training:False}\n",
    "                  pred_indexs = sess.run(topk_index, feed_dict)\n",
    "                  pred_list += pred_indexs.tolist()\n",
    "                  #only predict one next item\n",
    "                  single_target = [item[0] for item in next_target]\n",
    "                  next_list += single_target\n",
    "              hr50 = Metric_HR(next_list,pred_list,50)\n",
    "              mrr = Metric_MRR(next_list,pred_list)\n",
    "              print(\" epoch {},  mean_loss{:g}, test HR@50: {:g}, test MRR: {:g}\"\n",
    "                    .format(epoch + 1, mean_cost,hr50,mrr))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main([])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. S. Zhang, W. Chen, and H. Lee, \"Next item recommendation with self-attention,\" in Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2020, pp. 1227–1236, doi: 10.1145/3397271.3401075.\n",
    "2. S. Ge, \"AttRec: A Recommender System with Self-Attention Mechanism,\" GitHub repository, 2020. [Online]. Available: https://github.com/slientGe/AttRec. [Accessed: Nov. 19, 2024]."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "RS",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
