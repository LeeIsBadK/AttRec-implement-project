{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Recommenders contributors.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template\n",
    "\n",
    "Title of the notebooks should be concise and it's at heading-1 level, i.e., with one \"#\" in the markdown code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right under the notebook title, a brief introduction of the notebook is placed. Usually this will be what technical/business problems that the technical contents in this notebook try to solve."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to set a version check for Recommenders."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Global settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heading-2 level is for each sections in the notebook. It starts from 0, where it is usually about global settings such as module imports, global variable definitions, etc. \n",
    "Name of the section starts with a capital letter. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module imports\n",
    "\n",
    "It is a good practice to add all the imports in the first cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\n",
      "Tensorflow version: 2.18.0\n",
      "AttRec module imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf.get_logger().setLevel('ERROR')  # only show error messages\n",
    "\n",
    "\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.datasets.python_splitters import python_chrono_split\n",
    "from recommenders.evaluation.python_evaluation import (\n",
    "    map, ndcg_at_k, precision_at_k, recall_at_k\n",
    ")\n",
    "\n",
    "from recommenders.models.attrec.attrec import AttRec\n",
    "from recommenders.models.attrec.dataIterator import DataIterator\n",
    "from recommenders.utils.constants import SEED as DEFAULT_SEED\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(\"AttRec module imported successfully!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the convenience of parameterizing notebook tests, tagging of \"parameters\" can be added to the cell such that variables in the cell can be found by `papermill` in testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "RECOMMENDERS_VERSION = \"0.1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 50\n",
    "\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "SEED = DEFAULT_SEED  # Set None for non-deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:02<00:00, 2.30kKB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3.0</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating  timestamp\n",
       "0     196     242     3.0  881250949\n",
       "1     186     302     3.0  891717742\n",
       "2      22     377     1.0  878887116\n",
       "3     244      51     2.0  880606923\n",
       "4     166     346     1.0  886397596"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_train_test(df, seq_counts=5, target_counts=3, save_dir='processed_data', is_Save=True):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train/test sets with user-item sequences.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the user-item interaction data file.\n",
    "        seq_counts (int): Length of input sequences.\n",
    "        target_counts (int): Number of items to predict.\n",
    "        save_dir (str): Directory to save the train/test data.\n",
    "        is_save (bool): Whether to save the datasets and metadata.\n",
    "\n",
    "    Returns:\n",
    "        train (pd.DataFrame): Training data.\n",
    "        test (pd.DataFrame): Testing data.\n",
    "        user_all_items (dict): Mapping of users to their full item interaction lists.\n",
    "        all_user_count (int): Total number of unique users.\n",
    "        all_item_count (int): Total number of unique items.\n",
    "        user_map (dict): Mapping of original user IDs to remapped IDs.\n",
    "        item_map (dict): Mapping of original item IDs to remapped IDs.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # # Load data\n",
    "    data_path = '/Users/leeisbadk/recommenders/examples/99_model_attrec/ml-100k/ml-100k/u.data'\n",
    "    data = pd.read_csv(data_path, sep='\\t', header=None, names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "    # Remap user and item IDs to start from 1\n",
    "    user_map = {uid: i for i, uid in enumerate(data['user_id'].unique())}\n",
    "    item_map = {iid: i for i, iid in enumerate(data['item_id'].unique())}\n",
    "    data['user_id'] = data['user_id'].map(user_map)\n",
    "    data['item_id'] = data['item_id'].map(item_map)\n",
    "\n",
    "    # Sort data by user and timestamp\n",
    "    data = data.sort_values(by=['user_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "    # Group data by user\n",
    "    user_sessions = data.groupby('user_id')['item_id'].apply(list).reset_index()\n",
    "    user_sessions.rename(columns={'item_id': 'item_list'}, inplace=True)\n",
    "\n",
    "    user_all_items = {}\n",
    "    train_users, train_seqs, train_targets = [], [], []\n",
    "    test_users, test_seqs, test_targets = [], [], []\n",
    "\n",
    "    for _, row in user_sessions.iterrows():\n",
    "        user = row['user_id']\n",
    "        items = row['item_list']\n",
    "        user_all_items[user] = items\n",
    "\n",
    "        # Create training sequences\n",
    "        for i in range(seq_counts, len(items) - target_counts):\n",
    "            seqs = items[i - seq_counts:i]\n",
    "            targets = items[i:i + target_counts]\n",
    "            train_users.append(user)\n",
    "            train_seqs.append(seqs)\n",
    "            train_targets.append(targets)\n",
    "\n",
    "        # Create testing sequence\n",
    "        if len(items) > seq_counts + target_counts:\n",
    "            test_seq = items[-seq_counts - target_counts:-target_counts]\n",
    "            test_target = items[-target_counts:]\n",
    "            test_users.append(user)\n",
    "            test_seqs.append(test_seq)\n",
    "            test_targets.append(test_target)\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    train = pd.DataFrame({'user': train_users, 'seq': train_seqs, 'target': train_targets})\n",
    "    test = pd.DataFrame({'user': test_users, 'seq': test_seqs, 'target': test_targets})\n",
    "\n",
    "    # Metadata\n",
    "    all_user_count = len(user_map)\n",
    "    all_item_count = len(item_map)\n",
    "\n",
    "    if is_Save:\n",
    "        # Save datasets\n",
    "        train.to_csv(os.path.join(save_dir, 'train.csv'), index=False)\n",
    "        test.to_csv(os.path.join(save_dir, 'test.csv'), index=False)\n",
    "\n",
    "        # Save mappings and metadata\n",
    "        with open(os.path.join(save_dir, 'info.pkl'), 'wb') as f:\n",
    "            pickle.dump(user_all_items, f, pickle.HIGHEST_PROTOCOL)\n",
    "            pickle.dump(all_user_count, f, pickle.HIGHEST_PROTOCOL)\n",
    "            pickle.dump(all_item_count, f, pickle.HIGHEST_PROTOCOL)\n",
    "            pickle.dump(user_map, f, pickle.HIGHEST_PROTOCOL)\n",
    "            pickle.dump(item_map, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print(f\"Train and test datasets saved in '{save_dir}'\")\n",
    "\n",
    "    return train, test, user_all_items, all_user_count, all_item_count, user_map, item_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(file_path='/Users/leeisbadk/Library/Jupyter/runtime/kernel-v313f62ff26472befe2c513c3af12f03d61c2dd95b.json', test_path='input/test.csv', train_path='input/train.csv', mode='train', w=0.3, num_epochs=30, sequence_length=5, target_length=3, neg_sample_count=10, item_count=1685, user_count=945, embedding_size=100, batch_size=256, learning_rate=0.01, keep_prob=0.5, l2_lambda=0.001, gamma=0.5, grad_clip=10, save_path='save_path/model1.ckpt')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--file_path', type=str, default='/Users/leeisbadk/recommenders/examples/99_model_attrec/ml-100k/ml-100k/u.data', help='training data dir')\n",
    "parser.add_argument('--test_path', type=str, default='input/test.csv', help='testing data dir')\n",
    "parser.add_argument('--train_path', type=str, default='input/train.csv', help='training data dir')\n",
    "parser.add_argument('--mode', type=str, default='train', help='train or test')\n",
    "parser.add_argument('--w', type=float, default=0.3, help='The final score is a weighted sum of them with the controlling factor ω')\n",
    "parser.add_argument('--num_epochs', type=int, default=30, help='number of epochs')\n",
    "parser.add_argument('--sequence_length', type=int, default=5, help='sequence length')\n",
    "parser.add_argument('--target_length', type=int, default=3, help='target length') ##ควรเป็น 3\n",
    "parser.add_argument('--neg_sample_count', type=int, default=10, help='number of negative sample')\n",
    "parser.add_argument('--item_count', type=int, default=1685, help='number of items')\n",
    "parser.add_argument('--user_count', type=int, default=945, help='number of user')\n",
    "parser.add_argument('--embedding_size', type=int, default=100, help='embedding size')\n",
    "parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-2, help='learning rate')\n",
    "parser.add_argument('--keep_prob', type=float, default=0.5, help='keep prob of dropout')\n",
    "parser.add_argument('--l2_lambda', type=float, default=1e-3, help='Regularization rate for l2')\n",
    "parser.add_argument('--gamma', type=float, default=0.5, help='gamma of the margin higle loss')\n",
    "parser.add_argument('--grad_clip', type=float, default=10, help='gradient clip to prevent from grdient to large')\n",
    "parser.add_argument('--save_path', type=str, default='save_path/model1.ckpt', help='the whole path to save the model')\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "print(FLAGS)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metric_HR(target_list, predict_list, num):\n",
    "    count = 0\n",
    "    for i in range(len(target_list)):\n",
    "        t = target_list[i]\n",
    "        preds = predict_list[i]\n",
    "        preds = preds[:num]\n",
    "        if t in preds:\n",
    "            count += 1\n",
    "    return count / len(target_list)\n",
    "\n",
    "def Metric_MRR(target_list, predict_list):\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(target_list)):\n",
    "        t = target_list[i]\n",
    "        preds = predict_list[i]\n",
    "        rank = preds.index(t) + 1\n",
    "        count += 1 / rank\n",
    "    return count / len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " make datasets\n",
      "       user                        seq            target\n",
      "0         0    [0, 289, 491, 380, 751]    [466, 522, 10]\n",
      "1         0  [289, 491, 380, 751, 466]    [522, 10, 672]\n",
      "2         0  [491, 380, 751, 466, 522]   [10, 672, 1045]\n",
      "3         0   [380, 751, 466, 522, 10]  [672, 1045, 649]\n",
      "4         0   [751, 466, 522, 10, 672]  [1045, 649, 377]\n",
      "...     ...                        ...               ...\n",
      "92451   942   [209, 10, 873, 935, 614]    [355, 158, 12]\n",
      "92452   942   [10, 873, 935, 614, 355]    [158, 12, 141]\n",
      "92453   942  [873, 935, 614, 355, 158]    [12, 141, 452]\n",
      "92454   942   [935, 614, 355, 158, 12]   [141, 452, 672]\n",
      "92455   942   [614, 355, 158, 12, 141]    [452, 672, 68]\n",
      "\n",
      "[92456 rows x 3 columns]\n",
      "     user                           seq            target\n",
      "0       0    [834, 438, 632, 656, 1006]   [947, 363, 521]\n",
      "1       1       [452, 899, 25, 246, 48]    [530, 145, 31]\n",
      "2       2     [758, 437, 458, 476, 368]    [769, 14, 305]\n",
      "3       3   [834, 215, 1092, 945, 1100]  [689, 1210, 525]\n",
      "4       4     [652, 175, 731, 265, 668]   [187, 252, 985]\n",
      "..    ...                           ...               ...\n",
      "938   938  [190, 634, 1172, 1119, 1067]  [1104, 415, 706]\n",
      "939   939     [534, 1074, 672, 59, 642]   [189, 722, 177]\n",
      "940   940       [276, 221, 6, 199, 389]   [288, 347, 148]\n",
      "941   941     [420, 404, 608, 157, 184]   [117, 585, 139]\n",
      "942   942      [355, 158, 12, 141, 452]     [672, 68, 24]\n",
      "\n",
      "[943 rows x 3 columns]\n",
      " load model and training\n",
      "Tensor(\"AttRec/attention/MatMul_1:0\", shape=(?, 5, 100), dtype=float32)\n",
      "pass\n",
      "Tensor(\"AttRec/Max:0\", shape=(?, 100), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732037766.321900 1361356 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "W0000 00:00:1732037766.503567 1361356 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1732037774.419466 1361356 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 1,  mean_loss0.133791, test HR@50: 0.420997, test MRR: 0.0483422\n",
      " epoch 2,  mean_loss0.0635938, test HR@50: 0.47508, test MRR: 0.0553399\n",
      " epoch 3,  mean_loss0.0547148, test HR@50: 0.493107, test MRR: 0.051918\n",
      " epoch 4,  mean_loss0.0521787, test HR@50: 0.509014, test MRR: 0.0594593\n",
      " epoch 5,  mean_loss0.0509994, test HR@50: 0.530223, test MRR: 0.0622902\n",
      " epoch 6,  mean_loss0.0501182, test HR@50: 0.532344, test MRR: 0.0599459\n",
      " epoch 7,  mean_loss0.0493966, test HR@50: 0.534464, test MRR: 0.0675514\n",
      " epoch 8,  mean_loss0.0488412, test HR@50: 0.514316, test MRR: 0.0663028\n",
      " epoch 9,  mean_loss0.0484448, test HR@50: 0.534464, test MRR: 0.0641826\n",
      " epoch 10,  mean_loss0.0480731, test HR@50: 0.534464, test MRR: 0.0651407\n",
      " epoch 11,  mean_loss0.0477825, test HR@50: 0.533404, test MRR: 0.0611846\n",
      " epoch 12,  mean_loss0.047418, test HR@50: 0.52492, test MRR: 0.0658963\n"
     ]
    }
   ],
   "source": [
    "from recommenders.models.attrec.attrec import AttRec\n",
    "from recommenders.models.attrec.dataIterator import DataIterator\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "\n",
    "def Metric_HR(target_list, predict_list, num):\n",
    "    count = 0\n",
    "    for i in range(len(target_list)):\n",
    "        t = target_list[i]\n",
    "        preds = predict_list[i]\n",
    "        preds = preds[:num]\n",
    "        if t in preds:\n",
    "            count += 1\n",
    "    return count / len(target_list)\n",
    "\n",
    "def Metric_MRR(target_list, predict_list):\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(target_list)):\n",
    "        t = target_list[i]\n",
    "        preds = predict_list[i]\n",
    "        rank = preds.index(t) + 1\n",
    "        count += 1 / rank\n",
    "    return count / len(target_list)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    data, num_users, num_items = df, df['userID'].nunique(), df['itemID'].nunique()\n",
    "    print(' make datasets')\n",
    "    train_data, test_data ,user_all_items, all_user_count\\\n",
    "        , all_item_count, user_map, item_map \\\n",
    "        = create_train_test(FLAGS.file_path, FLAGS.sequence_length, FLAGS.target_length, is_Save=False)\n",
    "    FLAGS.item_count = all_item_count\n",
    "    FLAGS.user_count = all_user_count\n",
    "    all_index = [i for i in range(FLAGS.item_count)]\n",
    "    print(train_data)\n",
    "    print(test_data)\n",
    "    print(' load model and training')\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "      with tf.compat.v1.Session() as sess:\n",
    "          #Load model\n",
    "          model = AttRec(FLAGS)\n",
    "          topk_index = model.predict(all_index,len(all_index))\n",
    "          total_loss = model.loss\n",
    "\n",
    "          #Add L2\n",
    "          # with tf.name_scope('l2loss'):\n",
    "          #     loss = model.loss\n",
    "          #     tv = tf.trainable_variables()\n",
    "          #     regularization_cost = FLAGS.l2_lambda * tf.reduce_sum([tf.nn.l2_loss(v) for v in tv])\n",
    "          #     total_loss = loss + regularization_cost\n",
    "\n",
    "          #Optimizer\n",
    "          global_step = tf.Variable(0, trainable=False)\n",
    "          update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "          with tf.control_dependencies(update_ops):\n",
    "              optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "              tvars = tf.trainable_variables()\n",
    "              grads, _ = tf.clip_by_global_norm(tf.gradients(total_loss, tvars), FLAGS.grad_clip)\n",
    "              grads_and_vars = tuple(zip(grads, tvars))\n",
    "              train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "          #Saver and initializer\n",
    "          saver = tf.train.Saver()\n",
    "          if FLAGS.mode == 'test':\n",
    "              saver.restore(sess, FLAGS.save_path)\n",
    "          else:\n",
    "              sess.run(tf.global_variables_initializer())\n",
    "\n",
    "          #Batch reader\n",
    "          trainIterator = DataIterator(data=train_data\n",
    "                                      , batch_size=FLAGS.batch_size\n",
    "                                      ,max_seq_length=FLAGS.batch_size\n",
    "                                      ,neg_count=FLAGS.neg_sample_count\n",
    "                                      ,all_items=all_index\n",
    "                                      ,user_all_items=user_all_items\n",
    "                                      ,shuffle=True)\n",
    "          testIterator = DataIterator(data=test_data\n",
    "                                      ,batch_size = FLAGS.batch_size\n",
    "                                      , max_seq_length=FLAGS.batch_size\n",
    "                                      , neg_count=FLAGS.neg_sample_count\n",
    "                                      , all_items=all_index\n",
    "                                      , user_all_items=user_all_items\n",
    "                                      , shuffle=False)\n",
    "          #Training and test for every epoch\n",
    "          for epoch in range(FLAGS.num_epochs):\n",
    "              cost_list = []\n",
    "              for train_input in trainIterator:\n",
    "                user, next_target, user_seq, sl, neg_seq = train_input\n",
    "\n",
    "                # Convert lists to NumPy arrays before checking shape\n",
    "                user_seq_array = np.array(user_seq)\n",
    "                neg_seq_array = np.array(neg_seq)\n",
    "                user_array = np.array(user)\n",
    "                next_target_array = np.array(next_target)\n",
    "                # Print shapes of relevant tensors\n",
    "                # print(\"Shape of user_seq:\", user_seq_array.shape)\n",
    "                # print(\"Shape of neg_seq:\", neg_seq_array.shape)\n",
    "                # print(\"Shape of hist_seq:\", user_seq_array.shape)  #tis the issue\n",
    "                feed_dict = {model.u_p: user, model.next_p: next_target, model.sl: sl,\n",
    "                            model.hist_seq: user_seq, model.neg_p: neg_seq,\n",
    "                            model.keep_prob:FLAGS.keep_prob,model.is_Training:True}\n",
    "\n",
    "                _, step, cost = sess.run([train_op, global_step, total_loss], feed_dict)\n",
    "                cost_list.append(np.mean(cost))\n",
    "              mean_cost = np.mean(cost_list)\n",
    "              saver.save(sess, FLAGS.save_path)\n",
    "\n",
    "              pred_list = []\n",
    "              next_list = []\n",
    "              # test and cal hr50 and mrr\n",
    "              for test_input in testIterator:\n",
    "                  user, next_target, user_seq, sl, neg_seq = test_input\n",
    "                  feed_dict = {model.u_p: user, model.next_p: next_target, model.sl: sl,\n",
    "                              model.hist_seq: user_seq,model.keep_prob:1.0\n",
    "                              ,model.is_Training:False}\n",
    "                  pred_indexs = sess.run(topk_index, feed_dict)\n",
    "                  pred_list += pred_indexs.tolist()\n",
    "                  #only predict one next item\n",
    "                  single_target = [item[0] for item in next_target]\n",
    "                  next_list += single_target\n",
    "              hr50 = Metric_HR(next_list,pred_list,50)\n",
    "              mrr = Metric_MRR(next_list,pred_list)\n",
    "              print(\" epoch {},  mean_loss{:g}, test HR@50: {:g}, test MRR: {:g}\"\n",
    "                    .format(epoch + 1, mean_cost,hr50,mrr))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Section1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the sections can be hierarchical. Level numbers are connect by \".\". "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Sub-section1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \n",
    "1. The Python codes in the notebook should follow PEP standard.\n",
    "2. The code should be formatted with Black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_version(version, current_version):\n",
    "    v1_parts = version.split(\".\")\n",
    "    v2_parts = current_version.split(\".\")\n",
    "\n",
    "    # Pad the version parts with zeros to ensure equal length\n",
    "    max_parts = max(len(v1_parts), len(v2_parts))\n",
    "    v1_parts.extend([\"0\"] * (max_parts - len(v1_parts)))\n",
    "    v2_parts.extend([\"0\"] * (max_parts - len(v2_parts)))\n",
    "\n",
    "    for v1, v2 in zip(v1_parts, v2_parts):\n",
    "        if int(v1) <= int(v2):\n",
    "            print(f\"{version} is older than {current_version}\")\n",
    "            return True\n",
    "        elif int(v1) > int(v2):\n",
    "            print(f\"Error: {version} is newer than {current_version}\")\n",
    "            raise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.1 is older than 1.2.0\n"
     ]
    }
   ],
   "source": [
    "checked_version = check_version(RECOMMENDERS_VERSION, recommenders.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes in a notebook are tested with `store_metadata`. Below the example shows how to record a variable for testing purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/notebook_utils.json+json": {
       "data": true,
       "encoder": "json",
       "name": "checked_version"
      }
     },
     "metadata": {
      "notebook_utils": {
       "data": true,
       "display": false,
       "name": "checked_version"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "store_metadata(\"checked_version\", checked_version)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Sub-sub-section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Sub-section2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Section2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "It is highly encouraged to have references for technical explanations in the notebooks for people to easily understand theories and reproduce codes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "    \n",
    "1. Jianxu Lian et al, \"xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems\", Proc. ACM KDD, London, UK, 2018, pp. 1754-1763.\n",
    "2. PySpark MLlib evaluation metrics, url: https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this section, which is not the body sections of the notebook, does not have to be numbered in section name. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "RS",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
