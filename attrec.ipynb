{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AttRec\n",
    "\n",
    "Next Item Recommendation with Self-Attention (Shuai Zhang et al.) \n",
    "[https://arxiv.org/pdf/1808.06414](https://arxiv.org/pdf/1808.06414)\n",
    "\n",
    "Implement in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AttRec algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The proposed self-attentive sequential recommendation model, named **AttRec**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the user's short-term intents can be derived from her recent $L$ (e.g., 5, 10) interactions. Assuming each item can be represented with a $d$-dimension embedding vector. Let $X \\in \\mathbb{R}^{N \\times d}$ denote the embedding representations for the whole item set. The latest $L$ items (i.e., from item $t - L + 1$ to item $t$) are stacked together in sequence to get the following matrix:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "X_t^u = \n",
    "\\begin{bmatrix}\n",
    "X_{(t-L+1)1} & X_{(t-L+1)2} & \\cdots & X_{(t-L+1)d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "X_{(t-1)1} & X_{(t-1)2} & \\cdots & X_{(t-1)d} \\\\\n",
    "X_{t1} & X_{t2} & \\cdots & X_{td}\n",
    "\\end{bmatrix}\n",
    "\\tag{1}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the latest $L$ items is a subset of $H^u$. **Query**, **key**, and **value** for user $u$ at time step $t$ in the self-attention model are equal to $X_t^u$.\n",
    "\n",
    "First, we project **query** and **key** to the same space through a non-linear transformation with shared parameters:\n",
    "\n",
    "$$\n",
    "Q' = \\text{ReLU}(X_t^u W_Q) \\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K' = \\text{ReLU}(X_t^u W_K) \\tag{3}\n",
    "$$\n",
    "\n",
    "The weight matrices $W_Q \\in \\mathbb{R}^{d \\times d}$ and $W_K \\in \\mathbb{R}^{d \\times d}$ are used for projecting the **query** and **key**, respectively. The activation function ReLU is applied to introduce non-linearity into the attention mechanism. The affinity matrix is computed as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s_t^u = \\text{softmax}\\left(\\frac{Q'K'^T}{\\sqrt{d}}\\right) \\tag{4}\n",
    "$$\n",
    "\n",
    "The output is an $L \\times L$ affinity matrix (or attention map) representing the similarity between the $L$ items. The term $\\sqrt{d}$ is included to scale the dot-product attention, preventing it from growing excessively large when $d$ is set to a high value (e.g., 100). This scaling helps to reduce issues caused by extremely small gradients. Additionally, a masking operation is applied to the diagonal of the affinity matrix before the softmax step, ensuring that identical vectors for **query** and **key** do not result in high matching scores.\n",
    "\n",
    "Next, the **value** is kept unchanged, equal to $X_t^u$. Unlike other approaches, where the **value** is often transformed linearly, this model benefits from using identity mapping for **value**. In various applications, such as natural language processing or image recognition, the **value** typically consists of pre-trained embeddings (e.g., word or image features). In this model, the **value** directly reflects parameters specific to the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding linear (or nonlinear) transformation will increase the difficulty in seeing the actual parameters. Query and key are used as auxiliary factors so that they are not as sensitive as value to transformations. Finally, the affinity matrix and the value are multiplied to form the final weighted output of the self-attention module. \n",
    "\n",
    "$$a_{t}^{u}=s_{t}^{u}X_{t}^{u}  \\tag{5}$$\n",
    "\n",
    "The attentive output $a_{t}^{u}\\in\\mathcal{R}^{L\\times d}$ can be interpreted as user's short-term intent representations. To learn a single attentive representation, we average the L self-attention representations, representing user temporal intent. Other aggregation operations like sum, max, and min can also be used, and we will evaluate their effectiveness in our experiments. \n",
    "\n",
    "$$m_{t}^{u}=\\frac{1}{L}\\sum_{l=1}^{L}a_{tl}^{u}  \\tag{6}$$\n",
    "\n",
    "The above attention model lacks time signals. Without them, the input becomes a bag of embeddings and fails to capture sequential patterns. Inspired by the Transformer, we propose to include time information in the query and key using positional embeddings. \n",
    "\n",
    "We employ a geometric sequence of timescales to add sinusoids of different frequencies to the input. The time embedding (TE) consists of two sinusoidal signals defined as follows:\n",
    "\n",
    "$$TE(t,2i)=sin(t/10000^{2i/d})  \\tag{7}$$\n",
    "\n",
    "\n",
    "$$TE(t,2i+1)=cos(t/10000^{2i/d}) \\tag{8}$$\n",
    "\n",
    "Here, t represents the time step and i is the dimension. The TE is simply added to query and key before the nonlinear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 User Long-Term Preference Modelling\n",
    "\n",
    "To better understand and predict user behavior, we need to consider not only their immediate preferences but also their long-term tastes. Similar to how latent factor models work, we can assign each user and item a set of hidden features or representations. These representations, denoted as $U$ and $V$ respectively, capture the underlying characteristics of users and items.\n",
    "\n",
    "We could use the dot product to model how much a user likes an item, similar to how latent factor models work. However, recent research suggests that this approach can lead to suboptimal results because it doesn't always follow the expected mathematical properties of distance measures. To address this, we use Euclidean distance to measure how similar a user and an item are.\n",
    "\n",
    "$$\n",
    "||U_u - V_i||_2^2 \\tag{9}\n",
    "$$\n",
    "\n",
    "The distance is expected to be small if user $u$ liked the item $i$, and large otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Model Learning\n",
    "\n",
    "**Objective Function:** Given the user's current interests and their long-term preferences, our goal is to predict the next item they will interact with. To ensure consistency, we use Euclidean distance to measure both short-term and long-term similarity, and then combine them to create a final recommendation score. \n",
    "\n",
    "$$y_{t+1}^{u}=\\omega||U_{u}-V_{\\mathcal{H}_{t+1}}^{u}||_{2}^{2}+(1-\\omega)||m_{t}^{u}-X_{t+1}^{u}||_{2}^{2} \\tag{10}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend our model to predict multiple items instead of just one. This allows us to capture sequential patterns and user behavior that involves skipping items. We denote the next T items that the user liked as $T^+$. To train our model, we use a pairwise ranking approach. This involves sampling T negative items that the user did not interact with (or disliked) and denoting this set as $T^-$. \n",
    "\n",
    "We use a margin-based hinge loss to encourage discrimination between positive and negative user-item pairs:\n",
    "\n",
    "$$\n",
    "L(\\Theta) = \\sum_{(u,i) \\in T^+} \\sum_{(u,j) \\in T^-} [y_{i}^u + \\gamma - y_{j}^u]_+ + \\lambda ||\\Theta||_2^2 \\tag{11}\n",
    "$$\n",
    "\n",
    "Here, $\\Theta = \\{X, V, U, W_Q, W_K\\}$ represents the model parameters. $y$ is the margin that separates positive and negative pairs. We use $l_2$ loss to control model complexity. Dropout can be applied to the nonlinear layer of the self-attention module. \n",
    "\n",
    "To handle sparse datasets, we can use norm clipping to constrain the parameters $X$, $V$, and $U$ to a unit Euclidean ball:\n",
    "\n",
    "$$\n",
    "||X||_2 \\leq 1, ||V||_2 \\leq 1, ||U||_2 \\leq 1 \\tag{12}\n",
    "$$\n",
    "\n",
    "This regularization approach helps alleviate the curse of dimensionality and prevents data points from spreading too widely."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\n",
      "Tensorflow version: 2.18.0\n",
      "AttRec module imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf.get_logger().setLevel('ERROR')  # only show error messages\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.models.attrec.attrec import AttRec\n",
    "from recommenders.models.attrec.dataIterator import DataIterator\n",
    "from recommenders.models.attrec.create_train_test import create_train_test\n",
    "from recommenders.utils.constants import SEED as DEFAULT_SEED\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(\"AttRec module imported successfully!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 50\n",
    "\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "SEED = DEFAULT_SEED  # Set None for non-deterministic results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:02<00:00, 1.90kKB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3.0</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating  timestamp\n",
       "0     196     242     3.0  881250949\n",
       "1     186     302     3.0  891717742\n",
       "2      22     377     1.0  878887116\n",
       "3     244      51     2.0  880606923\n",
       "4     166     346     1.0  886397596"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(file_path='/Users/leeisbadk/Library/Jupyter/runtime/kernel-v3b6e6cf3dd941b0ab5ee4c1225be15d1ad3028b69.json', test_path='input/test.csv', train_path='input/train.csv', mode='train', w=0.3, num_epochs=30, sequence_length=5, target_length=3, neg_sample_count=10, item_count=1685, user_count=945, embedding_size=100, batch_size=256, learning_rate=0.01, keep_prob=0.5, l2_lambda=0.001, gamma=0.5, grad_clip=10, save_path='save_path/model1.ckpt')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--file_path', type=str, default='/Users/leeisbadk/recommenders/examples/99_model_attrec/ml-100k/ml-100k/u.data', help='training data dir')\n",
    "parser.add_argument('--test_path', type=str, default='input/test.csv', help='testing data dir')\n",
    "parser.add_argument('--train_path', type=str, default='input/train.csv', help='training data dir')\n",
    "parser.add_argument('--mode', type=str, default='train', help='train or test')\n",
    "parser.add_argument('--w', type=float, default=0.3, help='The final score is a weighted sum of them with the controlling factor ω')\n",
    "parser.add_argument('--num_epochs', type=int, default=30, help='number of epochs')\n",
    "parser.add_argument('--sequence_length', type=int, default=5, help='sequence length')\n",
    "parser.add_argument('--target_length', type=int, default=3, help='target length')\n",
    "parser.add_argument('--neg_sample_count', type=int, default=10, help='number of negative sample')\n",
    "parser.add_argument('--item_count', type=int, default=1685, help='number of items')\n",
    "parser.add_argument('--user_count', type=int, default=945, help='number of user')\n",
    "parser.add_argument('--embedding_size', type=int, default=100, help='embedding size')\n",
    "parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-2, help='learning rate')\n",
    "parser.add_argument('--keep_prob', type=float, default=0.5, help='keep prob of dropout')\n",
    "parser.add_argument('--l2_lambda', type=float, default=1e-3, help='Regularization rate for l2')\n",
    "parser.add_argument('--gamma', type=float, default=0.5, help='gamma of the margin higle loss')\n",
    "parser.add_argument('--grad_clip', type=float, default=10, help='gradient clip to prevent from grdient to large')\n",
    "parser.add_argument('--save_path', type=str, default='save_path/model1.ckpt', help='the whole path to save the model')\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "print(FLAGS)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metric_HR(target_list, predict_list, num):\n",
    "    count = 0\n",
    "    for i in range(len(target_list)):\n",
    "        t = target_list[i]\n",
    "        preds = predict_list[i]\n",
    "        preds = preds[:num]\n",
    "        if t in preds:\n",
    "            count += 1\n",
    "    return count / len(target_list)\n",
    "\n",
    "def Metric_MRR(target_list, predict_list):\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(target_list)):\n",
    "        t = target_list[i]\n",
    "        preds = predict_list[i]\n",
    "        rank = preds.index(t) + 1\n",
    "        count += 1 / rank\n",
    "    return count / len(target_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " make datasets\n",
      "Train and test datasets saved in 'processed_data'\n",
      "       user                        seq            target\n",
      "0         0    [0, 289, 491, 380, 751]    [466, 522, 10]\n",
      "1         0  [289, 491, 380, 751, 466]    [522, 10, 672]\n",
      "2         0  [491, 380, 751, 466, 522]   [10, 672, 1045]\n",
      "3         0   [380, 751, 466, 522, 10]  [672, 1045, 649]\n",
      "4         0   [751, 466, 522, 10, 672]  [1045, 649, 377]\n",
      "...     ...                        ...               ...\n",
      "92451   942   [209, 10, 873, 935, 614]    [355, 158, 12]\n",
      "92452   942   [10, 873, 935, 614, 355]    [158, 12, 141]\n",
      "92453   942  [873, 935, 614, 355, 158]    [12, 141, 452]\n",
      "92454   942   [935, 614, 355, 158, 12]   [141, 452, 672]\n",
      "92455   942   [614, 355, 158, 12, 141]    [452, 672, 68]\n",
      "\n",
      "[92456 rows x 3 columns]\n",
      "     user                           seq            target\n",
      "0       0    [834, 438, 632, 656, 1006]   [947, 363, 521]\n",
      "1       1       [452, 899, 25, 246, 48]    [530, 145, 31]\n",
      "2       2     [758, 437, 458, 476, 368]    [769, 14, 305]\n",
      "3       3   [834, 215, 1092, 945, 1100]  [689, 1210, 525]\n",
      "4       4     [652, 175, 731, 265, 668]   [187, 252, 985]\n",
      "..    ...                           ...               ...\n",
      "938   938  [190, 634, 1172, 1119, 1067]  [1104, 415, 706]\n",
      "939   939     [534, 1074, 672, 59, 642]   [189, 722, 177]\n",
      "940   940       [276, 221, 6, 199, 389]   [288, 347, 148]\n",
      "941   941     [420, 404, 608, 157, 184]   [117, 585, 139]\n",
      "942   942      [355, 158, 12, 141, 452]     [672, 68, 24]\n",
      "\n",
      "[943 rows x 3 columns]\n",
      " load model and training\n",
      "Tensor(\"AttRec/attention/MatMul_1:0\", shape=(?, 5, 100), dtype=float32)\n",
      "pass\n",
      "Tensor(\"AttRec/Max:0\", shape=(?, 100), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732095993.559160 1959595 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1732096001.458769 1959595 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 1,  mean_loss0.13587, test HR@50: 0.424178, test MRR: 0.0508215\n",
      " epoch 2,  mean_loss0.0641207, test HR@50: 0.460233, test MRR: 0.0531281\n",
      " epoch 3,  mean_loss0.0547577, test HR@50: 0.495228, test MRR: 0.0573095\n",
      " epoch 4,  mean_loss0.0518686, test HR@50: 0.504772, test MRR: 0.0623951\n",
      " epoch 5,  mean_loss0.0509257, test HR@50: 0.515376, test MRR: 0.0631909\n",
      " epoch 6,  mean_loss0.0499196, test HR@50: 0.520679, test MRR: 0.0558318\n",
      " epoch 7,  mean_loss0.0491972, test HR@50: 0.5228, test MRR: 0.0592131\n",
      " epoch 8,  mean_loss0.0488362, test HR@50: 0.531283, test MRR: 0.0646016\n",
      " epoch 9,  mean_loss0.0484905, test HR@50: 0.532344, test MRR: 0.0645264\n",
      " epoch 10,  mean_loss0.0481041, test HR@50: 0.528102, test MRR: 0.0627065\n",
      " epoch 11,  mean_loss0.0476032, test HR@50: 0.535525, test MRR: 0.0681188\n",
      " epoch 12,  mean_loss0.0478272, test HR@50: 0.52492, test MRR: 0.0649222\n",
      " epoch 13,  mean_loss0.0474808, test HR@50: 0.535525, test MRR: 0.0664004\n",
      " epoch 14,  mean_loss0.0474026, test HR@50: 0.525981, test MRR: 0.0627485\n",
      " epoch 15,  mean_loss0.0472075, test HR@50: 0.532344, test MRR: 0.0615461\n",
      " epoch 16,  mean_loss0.0469806, test HR@50: 0.529162, test MRR: 0.0609283\n",
      " epoch 17,  mean_loss0.0469413, test HR@50: 0.537646, test MRR: 0.066534\n",
      " epoch 18,  mean_loss0.0467483, test HR@50: 0.519618, test MRR: 0.0683959\n",
      " epoch 19,  mean_loss0.0464225, test HR@50: 0.516437, test MRR: 0.0639053\n",
      " epoch 20,  mean_loss0.0466174, test HR@50: 0.518558, test MRR: 0.0648012\n",
      " epoch 21,  mean_loss0.046269, test HR@50: 0.517497, test MRR: 0.0633134\n",
      " epoch 22,  mean_loss0.0462581, test HR@50: 0.5228, test MRR: 0.0632863\n",
      " epoch 23,  mean_loss0.0461688, test HR@50: 0.493107, test MRR: 0.0643247\n",
      " epoch 24,  mean_loss0.0460265, test HR@50: 0.518558, test MRR: 0.061747\n",
      " epoch 25,  mean_loss0.0457744, test HR@50: 0.510074, test MRR: 0.0629617\n",
      " epoch 26,  mean_loss0.0454674, test HR@50: 0.515376, test MRR: 0.0651968\n",
      " epoch 27,  mean_loss0.0457382, test HR@50: 0.503712, test MRR: 0.0650246\n",
      " epoch 28,  mean_loss0.0455853, test HR@50: 0.511135, test MRR: 0.0614169\n",
      " epoch 29,  mean_loss0.0455785, test HR@50: 0.506893, test MRR: 0.0614637\n",
      " epoch 30,  mean_loss0.045548, test HR@50: 0.510074, test MRR: 0.0635944\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "def main(args):\n",
    "    data, num_users, num_items = df, df['userID'].nunique(), df['itemID'].nunique()\n",
    "    print(' make datasets')\n",
    "    train_data, test_data ,user_all_items, all_user_count\\\n",
    "        , all_item_count, user_map, item_map \\\n",
    "        = create_train_test(data, FLAGS.sequence_length, FLAGS.target_length, is_Save=True)\n",
    "    FLAGS.item_count = all_item_count\n",
    "    FLAGS.user_count = all_user_count\n",
    "    all_index = [i for i in range(FLAGS.item_count)]\n",
    "    print(train_data)\n",
    "    print(test_data)\n",
    "    print(' load model and training')\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "      with tf.compat.v1.Session() as sess:\n",
    "          #Load model\n",
    "          model = AttRec(FLAGS)\n",
    "          topk_index = model.predict(all_index,len(all_index))\n",
    "          total_loss = model.loss\n",
    "\n",
    "          #Add L2\n",
    "          # with tf.name_scope('l2loss'):\n",
    "          #     loss = model.loss\n",
    "          #     tv = tf.trainable_variables()\n",
    "          #     regularization_cost = FLAGS.l2_lambda * tf.reduce_sum([tf.nn.l2_loss(v) for v in tv])\n",
    "          #     total_loss = loss + regularization_cost\n",
    "\n",
    "          #Optimizer\n",
    "          global_step = tf.Variable(0, trainable=False)\n",
    "          update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "          with tf.control_dependencies(update_ops):\n",
    "              optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "              tvars = tf.trainable_variables()\n",
    "              grads, _ = tf.clip_by_global_norm(tf.gradients(total_loss, tvars), FLAGS.grad_clip)\n",
    "              grads_and_vars = tuple(zip(grads, tvars))\n",
    "              train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "          #Saver and initializer\n",
    "          saver = tf.train.Saver()\n",
    "          if FLAGS.mode == 'test':\n",
    "              saver.restore(sess, FLAGS.save_path)\n",
    "          else:\n",
    "              sess.run(tf.global_variables_initializer())\n",
    "\n",
    "          #Batch reader\n",
    "          trainIterator = DataIterator(data=train_data\n",
    "                                      , batch_size=FLAGS.batch_size\n",
    "                                      ,max_seq_length=FLAGS.batch_size\n",
    "                                      ,neg_count=FLAGS.neg_sample_count\n",
    "                                      ,all_items=all_index\n",
    "                                      ,user_all_items=user_all_items\n",
    "                                      ,shuffle=True)\n",
    "          testIterator = DataIterator(data=test_data\n",
    "                                      ,batch_size = FLAGS.batch_size\n",
    "                                      , max_seq_length=FLAGS.batch_size\n",
    "                                      , neg_count=FLAGS.neg_sample_count\n",
    "                                      , all_items=all_index\n",
    "                                      , user_all_items=user_all_items\n",
    "                                      , shuffle=False)\n",
    "          #Training and test for every epoch\n",
    "          for epoch in range(FLAGS.num_epochs):\n",
    "              cost_list = []\n",
    "              for train_input in trainIterator:\n",
    "                user, next_target, user_seq, sl, neg_seq = train_input\n",
    "\n",
    "                # Convert lists to NumPy arrays before checking shape\n",
    "                user_seq_array = np.array(user_seq)\n",
    "                neg_seq_array = np.array(neg_seq)\n",
    "                user_array = np.array(user)\n",
    "                next_target_array = np.array(next_target)\n",
    "                # Print shapes of relevant tensors\n",
    "                # print(\"Shape of user_seq:\", user_seq_array.shape)\n",
    "                # print(\"Shape of neg_seq:\", neg_seq_array.shape)\n",
    "                # print(\"Shape of hist_seq:\", user_seq_array.shape)  #tis the issue\n",
    "                feed_dict = {model.u_p: user, model.next_p: next_target, model.sl: sl,\n",
    "                            model.hist_seq: user_seq, model.neg_p: neg_seq,\n",
    "                            model.keep_prob:FLAGS.keep_prob,model.is_Training:True}\n",
    "\n",
    "                _, step, cost = sess.run([train_op, global_step, total_loss], feed_dict)\n",
    "                cost_list.append(np.mean(cost))\n",
    "              mean_cost = np.mean(cost_list)\n",
    "              saver.save(sess, FLAGS.save_path)\n",
    "\n",
    "              pred_list = []\n",
    "              next_list = []\n",
    "              # test and cal hr50 and mrr\n",
    "              for test_input in testIterator:\n",
    "                  user, next_target, user_seq, sl, neg_seq = test_input\n",
    "                  feed_dict = {model.u_p: user, model.next_p: next_target, model.sl: sl,\n",
    "                              model.hist_seq: user_seq,model.keep_prob:1.0\n",
    "                              ,model.is_Training:False}\n",
    "                  pred_indexs = sess.run(topk_index, feed_dict)\n",
    "                  pred_list += pred_indexs.tolist()\n",
    "                  #only predict one next item\n",
    "                  single_target = [item[0] for item in next_target]\n",
    "                  next_list += single_target\n",
    "              hr50 = Metric_HR(next_list,pred_list,50)\n",
    "              mrr = Metric_MRR(next_list,pred_list)\n",
    "              print(\" epoch {},  mean_loss{:g}, test HR@50: {:g}, test MRR: {:g}\"\n",
    "                    .format(epoch + 1, mean_cost,hr50,mrr))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main([])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. S. Zhang, W. Chen, and H. Lee, \"Next item recommendation with self-attention,\" in Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2020, pp. 1227–1236, doi: 10.1145/3397271.3401075.\n",
    "2. S. Ge, \"AttRec: A Recommender System with Self-Attention Mechanism,\" GitHub repository, 2020. [Online]. Available: https://github.com/slientGe/AttRec. [Accessed: Nov. 19, 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Recommenders contributors.</i> [GitHub](https://github.com/recommenders-team/recommenders)\n",
    "\n",
    "<i>Licensed under the MIT License.</i>\n",
    "\n",
    "<i>Implementation repo: [GitHub](https://github.com/LeeIsBadK/AttRec-implement-project) </i>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "RS",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
